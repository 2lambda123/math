\part{Introduction}


\chapter{Overview}

\noindent
This document is both a user's guide and a reference manual for
\Stan's probabilistic modeling language.  This introductory chapter
provides a high-level overview of \Stan.  The next chapter provides a
hands-on quick-start guide showing how \Stan works in practice.
Installation instructions are in \refappendix{install}. The remaining
parts of this document include a practically-oriented user's guide for
programming models and a detailed reference manual for \Stan's
modeling language and associated programs and data formats.

\section{\Stan Programs}

A \Stan program defines a statistical model through a conditional
probability function $p(\theta|y;x)$, where $\theta$ is a sequence of
modeled unknown values (e.g., model parameters, latent variables, missing
data, future predictions), $y$ is a sequence of modeled known 
values, and $x$ is a sequence of unmodeled predictors and constants
(e.g., sizes, hyperparameters).

\Stan programs consist of variable type declarations and statments.
Variable types include constrained and unconstrained integer, scalar,
vector, and matrix types, as well as (multidimensional) arrays of
other types.  Variables are declared in blocks corresponding to the
variable's use: data, transformed data, parameter, transformed
parameter, or generated quantity.  Unconstrained local variables may
be declared within statement blocks.

Statements in \Stan are interpreted imperatively, so their order
matters.  Atomic statements involve the assignment of a value to a
variable.  Sequences of statements (and optionally local variable
declarations) may be organized into a block.  \Stan also provides bounded
for-each loops of the sort used in \R and \BUGS.

The transformed data, transformed parameter, and generated quantities
blocks contain statements defining the variables declared in their
blocks.  A special model block consists of statements defining the log
probability for the model.

Within the model block, \BUGS-style sampling notation may be used as
shorthand for incrementing an underlying log probablity variable, the
value of which defines the log probability function.  The log
probability variable may also be accessed directly, allowing
user-defined probability functions and Jacobians of transforms.


\section{Compiling and Running \Stan Programs}

A \Stan program is first compiled to a \Cpp program by the \Stan
compiler \stanc, then the \Cpp program compiled to a self-contained
platform-specific executable.  \Stan can generate executables for
various flavors of Windows, Mac OS X, and Linux.%
%
\footnote{A \Stan program may also be compiled to a dynamically
  linkable object file for use in a higher-level scripting language
  such as \R or Python.}
%
Running the \Stan executable for a model first reads in and validates
the known values $y$ and $x$, then generates a sequence of
(non-independent) identically distributed samples $\theta^{(1)},
\theta^{(2)}, \ldots$, each of which has the marginal distribution
$p(\theta|y;x)$.


\section{\Stan's Samplers}

For continuous parameters, \Stan uses Hamiltonian Monte Carlo (\HMC)
sampling \citep{Duane:1987, Neal:1994, Neal:2011}, a form of Markov chain Monte
Carlo (\MCMC) sampling \citep{Metropolis:1953}.  \Stan 1.0 can only sample discrete parameters
on which no other parameters depend, such as simulated values.
\refchapter{mixture-modeling} discusses how finite discrete parameters
can be summed out of models.

\HMC accelerates both convergence to the stationary distribution and
subsequent parameter exploration by using the gradient of the log
probabilty function.  The unknown quantity vector $\theta$ is
interpreted as the position of a fictional particle.  Each iteration
generates a random momentum and simulates the path of the particle
with potential energy determined the (negative) log probability
function.  Hamilton's decomposition shows that the gradient of this
potential determines change in momentum and the momentum determines
the change in position.  These continuous changes over time are
approximated using the leapfrog algorithm, which breaks the time into
discrete steps which are easily simulated.  A Metropolis reject step
is then applied to correct for any simulation error and ensure
detailed balance of the resulting Markov chain transitions
\citep{Metropolis:1953, Hastings:1970}.

Standard \HMC involves three ``tuning'' parameters to which its
behavior is quite sensitive.  \Stan's samplers allow these parameters
to be set by hand or set automatically without user intervention.

The first tuning parameter is a mass matrix for the fictional
particle.  \Stan can be configured to use a unit mass matrix or to
estimate a diagonal mass matrix during warmup; in the future, it will
also support a user-defined diagonal mass matrix.  Estimating the mass
matrix normalizes the scale of each element $\theta_k$ of the unknown
variable sequence $\theta$.

The other two tuning parameters set the temporal step size of the
discretization of the Hamiltonian and the total number of steps taken
per iteration.  \Stan can be configured with a user-specified step
size or it can estimate an optimal step size during warmup using dual
averaging \citep{Nesterov:2009, Hoffman-Gelman:2012}.  In either case, additional
randomization may be applied to draw the step size from an interval of
possible step sizes \citep{Neal:2011}.

\Stan can be set to use a specified number of steps, or it can
automatically adapt the number of steps during sampling using the
no-U-turn (\NUTS) sampler \citep{Hoffman-Gelman:2012}.  


\section{Convergence Monitoring and Effective Sample Size}

Samples in a Markov chain are only drawn with the marginal
distribution $p(\theta|y;x)$ after the chain has converged to its
equilibrium distribution.  There are several methods to test whether
an \MCMC method has failed to converge; unforunately, passing the
tests does not guarantee convergence.  The recommended method for
\Stan is to run multiple Markov chains each with different diffuse
initial parameter values, discard the warmup/adaptation samples, then
split the remainder of each chain in half and compute the potential
scale reduction statistic, $\hat{R}$ \citep{GelmanRubin:1992}.

When estimating a mean based on $M$ independent samples, the
estimation error is proportional to $1/\sqrt{M}$.  If the samples are
positively correlated, as they typically are when drawn using \MCMC
methods, the error is proportional to $1/\sqrt{\mbox{\sc ess}}$, where
{\sc ess} is the effective sample size.  Thus it is standard practice
to also monitor (an estimate of) the effective sample size of
parameters of interest in order to estimate the additional estimation
error due to correlated samples.





\section{Bayesian Inference and Monte Carlo Methods}

\Stan was developed to support full Bayesian inference.  Bayesian
inference is based in part on Bayes's rule,
\[
p(\theta|y;x) \propto p(y|\theta;x) \, p(\theta;x),
\]
which, in this unnormalized form, states that the posterior
probability $p(\theta|y;x)$ of parameters $\theta$ given data $y$ (and
constants $x$) is proportional (for fixed $y$ and $x$) to the
product of the likelihood function $p(y|\theta;x)$ and prior
$p(\theta;x)$.

For \Stan, Bayesian modeling involves coding the posterior probability
function up to a proportion, which Bayes's rule shows is equivalent to
modeling the product of the likelihood function and prior up to a
proportion.

Full Bayesian inference involves propagating the uncertainty in the
value of parameters $\theta$ modeled by the posterior $p(\theta|y;x)$.
This can be accomplished by basing inference on a sequence of samples
from the posterior using plug-in estimates for quantities of interest
such as posterior means, posterior intervals, predictions based on the
posterior such as event outcomes or the values of as yet unobserved
data.



\chapter{Getting Started}

\noindent
This chapter is designed to help users get acquainted with the overall
design of the \Stan language and calling \Stan from the command line.
Later chapters are devoted to expanding on the material in this
chapter with full reference documentation.

\section{Installation}

For installation information for the tools needed for Stan on Windows,
Mac, and Linux platforms, see \refappendix{install}.


\section{A Minimal Program}

Stan is distributed with several working models.  The simplest of
these is found in the following location relative to the top-level
distribution.
%
\begin{quote}
\begin{Verbatim}
src/models/basic_distributions/normal.stan
\end{Verbatim}
\end{quote}
%
The contents of this file are as follows.
%
\begin{quote}
\begin{Verbatim}
parameters {
  real y;
}
model {
  y ~ normal(0,1);
}
\end{Verbatim}
\end{quote}
%
The model's single parameter \code{y} is declared to take real values.
The probability model specifies that \code{y} has a normal
distribution with location 0 and scale 1.  This model will
sample a single unit normal variate.  

\section{Whitespace and Semicolons}

In \Stan, every variable declaration and atomic statement must be
terminated by a semicolon (\code{;}).  This is the convention followed
by programming languages such as \Cpp, but not the convention followed
by the statistical languages \R and \BUGS.

The reason for the \Cpp convention is to ensure that differences in
whitespace are not meaningful.  The following is a complete, legal
statement in \R and \BUGS.
%
\begin{quote}
\begin{Verbatim}
a <- b +
     c
\end{Verbatim}
\end{quote}
%
In contrast, the following is illegal in \R and \BUGS.
%
\begin{quote}
\begin{Verbatim}
a <- b
     + c
\end{Verbatim}
\end{quote}
%
The only difference is in the placement of whitespace.  In \Stan,
there is no whitespace-dependent behavior.  Neither of these is a
complete statement, whereas either one terminated with a semicolon is.
The second form is recommended for \Cpp and \Stan; it is the
convention adopted by most programming languages and in typesetting
mathematics because it allows easier visual identification of
continued lines.


\section{Unpacking \Stan}

The first step in using \Stan is unpacking the gzipped tar file into
its own directory.  This can be done with a system tool, or on the
command line with the \code{tar} command as follows.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> tar -xzf stan-1.0.0.tgz
...
> cd stan-1.0.0
\end{Verbatim}
\end{quote}
%
Throughout the documentation, \code{\$stan} will be used as a
placeholder for the top-level directory, \code{stan-1.0.0}, containing
\Stan.


\section{Building \Stan's Translator and Libraries}\label{build-stan-libs.section}

Once all the tools required for \Stan are installed (see
\refappendix{install}), \Stan's libraries must be built.  There is a
makefile supplied so that this can be done with one command.%
%
\footnote{The command to make the \Stan libraries can take ten minutes
  or more to compile on a single core at optimization level 3.  One
  way to speed this up a bit is to add the option \code{-j4} to the
  call to \code{make} to use four cores to do the work in parallel;
  other numbers of cores may also be specified.}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cd $stan
> make CC=g++ O=3 bin/libstan.a bin/stanc
\end{Verbatim}
\end{quote}
%
This will make the \Stan library archive, \code{libstan.a}, and the
model compiler \code{stanc}.  

The two options specify to use the
\code{g++} compiler for \Cpp and optimization level 3.  For a list
of compatible compilers and versions, see \refappendix{install}.




\section{Compiling  with \code{stanc}}

Starting at \Stan's home directory, the model may be compiled by 
the \Stan compiler, \stanc, into \Cpp code as follows.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cd $stan
> bin/stanc src/models/basic_distributions/normal.stan
\end{Verbatim}
%
\begin{Verbatim}
Model name=anon_model
Input file=src/models/basic_distributions/normal.stan
Output file=anon_model.cpp
\end{Verbatim}
\end{quote}
%
The output indicates the name of the model, here the default value
\code{anon\_model}, the input file from which the \Stan program is
read, here \code{normal.stan}, and the output file to which the
generated \Cpp code is written, here \code{anon\_model.cpp}.  See
\refchapter{stanc} for more documentation on the \stanc compiler.

\section{Compiling the Generated Code}

The file generated by \stanc must next be compiled with a \Cpp
compiler by linking to \Stan's source and library directories using
the {\tt -I} option of the compiler.  The following example 
uses the \clang compiler for \Cpp.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> g++ -O3 -Lbin -lstan -Isrc -Ilib/boost_1.50.0 \
  -Ilib/eigen_3.1.0 anon_model.cpp 
\end{Verbatim}
\end{quote}
%
The line-final backslashes indicate that the command is continued on
the next line.  This command invokes the \code{g++} compiler for \Cpp
to create a platform-specific executable in the default location,
which is {\tt a.out} by convention.  The \code{-O} option is for
optimization level 3, \code{-L} option specifies where to look for
compiled libraries, the \code{-l} option specifies the library to look
for, and the two \code{-I} libraries point to header files for \Stan
and its dependencies.

If all goes well, as above, there is no output to the console.  More
information about compiling the \Cpp code generated by \Stan may be
found in \refchapter{compiling-cpp}.  Installation information for
\Cpp compilers may be found in \refappendix{install}.

\section{Running the Sampler}

The executable resulting from compiling the generated \Cpp may be run
as follows.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./a.out
\end{Verbatim}
%
\begin{Verbatim}
STAN SAMPLING COMMAND
data = 
init = random initialization
samples = samples.csv
append_samples = 0
seed = 1331941513 (randomly generated)
chain_id=1 (default)
iter = 2000
warmup = 1000
thin = 1
leapfrog_steps = -1
max_treedepth = 10
epsilon = -1
epsilon_pm = 0
epsilon_adapt_off = 0
delta = 0.5
gamma = 0.05

Iteration: 2000 / 2000 [100%]  (Sampling)
\end{Verbatim}
\end{quote}
%
For more information on configuring the compiled sampler, see
\refchapter{stan-cmd}.

\section{Inspecting the Output}

The program indicates to the standard output that the samples are
written to \code{samples.csv}.  The first few lines of this file are
comments about aspects of the run; see \refchapter{stan-cmd} for more
information on these settings, most of which are user configurable
from the command line.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cat samples.csv
\end{Verbatim}
\begin{Verbatim}
# Samples Generated by Stan
#
# stan_version_major=alpha
# stan_version_minor=0
# stan_version_patch=0
# data=
# init=random initialization
# append_samples=0
# seed=1331941796
# chain_id=1
# iter=2000
# warmup=1000
# thin=1
# leapfrog_steps=-1
# max_treedepth=10
# epsilon=-1
# epsilon_pm=0
# delta=0.5
# gamma=0.05
...
\end{Verbatim}
\end{quote}
%
The ellipses notation, {\tt ...}, indicates that the output continues
beyond what's shown.  Here, what follows is the data in standard
comma-separate value ({\sc csv}) notation.
%
\begin{quote}
\begin{Verbatim}
...
lp__,treedepth__,y
-0.0126699,1,0.159185
-0.222796,1,-0.667527
-0.222796,1,-0.667527
-0.404457,1,-0.899397
...
\end{Verbatim}
\end{quote}
%
The first line consists of a header indicating the names of the
variables on the lines to follow, and each following line indicates a
single sampled value of the parameters.  The first column is reserved
for the (unnormalized) log probability (density) of the parameters,
with name {\tt lp\_\_} (the two trailing underscores are to prevent
name conflicts with user-defined model parameters).  The next values
are for reporting the behavior of the sampler.  In this case, the
\NUTS sampler was used, so there is a report of the depth of tree it
explored, with variable name {\tt treedepth\_\_}.  The remaining
values are parameters.  Here, the model has only one parameter, {\tt
  y}.  The first sampled value for {\tt y} is 0.159185, the second is
-0.667527, and so on.

Note that the second sampled value is repeated.  This is not a bug.
Rather, it is the behavior to expect from a sampler using a Metropolis
acceptance step for proposals, as \Stan's samplers \HMC and \NUTS do.
The reduction in inferential power due to the repetition is accounted
for in the effective sample size calculation, which adjusts for
autocorrelation in the sample sequence.

\section{Data}

\Stan allows data to be specified in programs, used in models, and
read into compiled \Stan programs. This section provides an example of
coding and running a \Stan program with data stored in a file in the
\SPLUS/\R dump format.

The \Stan program in 
\begin{quote}
\begin{Verbatim}
src/models/basic_estimators/bernoulli.stan
\end{Verbatim}
\end{quote}
can be used to estimate a Bernoulli parameter \code{theta} from
\code{N} binary observations.  The file contains the following code.
%
\begin{quote}
\begin{Verbatim}
data {
   int<lower=0> N;
   int<lower=0,upper=1> y[N];
}
parameters {
   real<lower=0,upper=1> theta;
}
model {
   for (n in 1:N)
      y[n] ~ bernoulli(theta);
}
\end{Verbatim}
\end{quote}
%
This program declares two data variables in its \code{data} block.
The first data variable, \code{N}, is an integer encoding the number
of observations.  The declaration \code{int<lower=0>} indicates that
\code{N} must take on non-negative values.  The second data variable,
\code{y}, is declared as \code{y[N]}, specifying that it is an array
of \code{N} values.  Each of these values has the declared type,
\code{int<lower=0,upper=1>}, an integer between 0 and 1 inclusive, i.e., a binary
value.  The \code{N} individual binary values in the array \code{y}
are accessed using standard array notation, indexing from 1, as \code{y[1]},
\code{y[2]}, ..., \code{y[N]}.

The \code{parameters} block declares a single parameter, \code{theta}.
The parameter \code{theta} is declared as type \code{real<lower=0,upper=1>},
contstraining it to continuous values between zero and one inclusive,
to ensure it satisfies the constraint placed on it by the probability
function \code{bernoulli} in the \code{model} block.

The \code{model} block consists of a for-loop for the data.   The loop is
specified so that the body is executed for values of \code{n} between
\code{1} and \code{N} inclusive.  The body here is a sampling
statement specifying that the variable \code{y[n]} is modeled as
having a Bernoulli distribution with chance of success \code{theta}.  

A sample data file for this program can be found in the file
\code{bernoulli.Rdata} in the same directory as the model.  This data file has
the following contents.
%
\begin{quote}
\begin{Verbatim}
N <- 10
y <- c(0,1,0,0,0,0,0,0,0,1)
\end{Verbatim}
\end{quote}
%
Here there
is a non-negative integer value for \code{N} and an array of length
\code{N} (i.e., 10) integer values between 0 and 1 inclusive.  The
array is coded using the same sequence notation as used by \R,  \code{c(...)}.
The dump format supported by \Stan is documented in \refchapter{dump}.

A data file must contain appropriate values for all of the data
variables declared in the \Stan program's \code{data} block.  The data
file may also include values not used by the \Stan program; the extra
variables take time and space to read into memory, but will not be
used by \Stan.

The data is not part of the compiled model.  Thus, the \Stan program
is compiled by \stanc and the \Cpp compiler in the same way as the
program in the previous section.  This time, the output model gets an
explicitly specified name.
%
%
\begin{center}
\begin{Verbatim}[fontshape=sl]
 > bin/stanc --name=bern src/models/basic_estimators/bernoulli.stan 
\end{Verbatim}
\begin{Verbatim}
 Model name=bern
 Input file=src/models/basic_estimators/bernoulli.stan
 Output file=bern.cpp
\end{Verbatim}
\end{center}
%
As before, the \Cpp compiler needs to be given the name of
generated file.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> g++ -O3 -Lbin -lstan -Isrc -Ilib/boost_1.50.0 \
  -Ilib/eigen_3.1.0 -o bern bern.cpp
\end{Verbatim}
\end{quote}
%
The new compiler option, \code{-o~bern}, specifies the name of the
executable file.  Now the code may be executed by calling its
executable with the data file specified.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./bern --data=src/models/basic_estimators/bernoulli.Rdata
\end{Verbatim}
\end{quote}
%
The command-line option \code{--data=} provides the location of the
data file.

\section{Proper and Improper Priors}

Priors are not required for variables in \Stan.  That is, not every
parameter needs to show up on the left-hand side of a sampling
statement in the model.  

\subsection{Implicit Uniform Priors}

The model in the previous section does not contain a sampling
statement for \code{theta}.  Not specifying a prior is equivalent to
specifying a uniform prior.  

A uniform prior is only proper if the parameter is bounded, as was the
case for \code{theta} in the previous section.  In this case, the
implicit uniform prior is proper because \code{theta} is bounded to a
finite interval.

Improper priors are also allowed in \Stan programs.  Improper priors
result if there is a parameter with support that is unbounded at least
on one side.  Parameters with support bounded on both sides are
equivalent to uniform priors.  In some cases, an improper prior may
lead to a proper posterior, but it is up to the user to guarantee that
constraints on the parameter(s) or the data ensure the propriety of
the posterior.

\subsection{Explicit Priors}

The uniform prior could have also been specified explicitly by adding the
following statement to the \code{model} block of the program.
%
\begin{quote}
\begin{Verbatim} 
theta ~ uniform(0,1);
\end{Verbatim}
\end{quote}
% 

\subsection{Conjugate Priors}

Because $\distro{Beta}(1,1)$ is the uniform distribution on $(0,1)$,
a uniform prior on \code{theta} may also be specified as follows.
%
\begin{quote}
\begin{Verbatim}
theta ~ beta(1,1);
\end{Verbatim}
\end{quote}
%
The beta distribution is conjugate to the Bernoulli, meaning that the
posterior is also a beta distribution.  In theory, the closed form of
the posterior would allow for more efficient sampling, but \Stan,
at least as of yet, does not make use of this information.  See
\refchapter{optimization} for information on how to code sufficient
statistics and conjugate priors explicitly in \Stan to speed this model up.





\section{Comments}

If the character pair \code{//} appears together on a line, that pair 
and every character up to but not including the end of line is 
ignored.  This is the style of comments used in C.  This is the 
preferred commenting style for line-based comments and for commenting 
out code.

The character \code{\#} behaves the same way as the character pair
\code{//}.  If \code{\#} appears on a line, that character and every
character up to but not including the end of line is ignored.  This is
the style of comments used in Python, R, and Unix shell scripts.  

\Stan also supports \Cpp comment style in which any content placed
between \code{/*} and \code{*/} is ignored along with the boundary
markers.  This is the preferred style for long documentation comments.
It is difficult to use this method to comment out code, because 
an internal close comment sequence, \code{*/}, may take precedence.


% \section{User-Defined Distributions and Functions}

% \Stan allows new distributions to be coded directly in the modeling
% language.  

%  in one of two ways, either
% directly in its modeling language (see
% \refchapter{custom-probability-functions}), or by extending the
% modeling language using \Cpp (see
% \refappendix{user-defined-functions}).  The latter is more efficient
% and also portable across models.

