\part{Introduction}


\chapter{Overview}

This document is both a user's guide and a reference manual for
\Stan's probabilistic modeling language.  This introductory chapter
provides a high-level view of \Stan's approach to Bayesian inference.
The next chapter provides a hands-on quick-start guide showing how
\Stan works in practice.  The remainder of this document contains a
practically-oriented user's guide for programming models in \Stan
followed by a detailed reference manual for \Stan and its associated
programs.

\section{What is \Stan?}

% START CUT & PASTE FROM abstract.tex
\Stan is a software package providing a strongly typed, imperative
language for specifying conditional probability models.  These models
specify the conditional probability function of unknown variables
given some known variables.  Model specifications are converted by
\Stan to \Cpp code for reading data, transforming parameters, and
calculating the conditional log probability function and its gradient.
This \Cpp code is then compiled to an executable for performing full
Bayesian inference by sampling from the posterior.  \Stan programs use
automatically tuned and adaptive Hamiltonian Monte Carlo for sampling.
% END CUT & PASTE


\section{\Stan's Models}

A \Stan program specifies a conditional probability function
\[
p(\theta|y;x),
\]
where
%
\begin{itemize}
\item $y$ is a vector of known values with a probabilistic model,
  such as observed outcomes,
\item $x$ is a vector of known values without a probabilistic
  model, such as size constants, predictors, fixed
  parameters, and fixed hyperparameters, and
\item $\theta$ is a vector of unknown values, such as estimated
  parameters, missing data, predictions, and simulated values.
\end{itemize}
%
Following the convention introduced by \BUGS, the elements of $x$ and
$y$ will be called ``data'' and the elements of $\theta$ called
``parameters.''  A compiled \Stan model is then given fixed values for
the data $x$ and $y$ from which it simulates a sequence of samples of
the parameters $\theta$ from the conditional distribution
$p(\theta|y;x)$.  See \refsection{intro-samplers} later in this
chapter for an introduction to \Stan's samplers.

\subsection{Normalization}

\Stan only requires the conditional probability function to be defined
up to an additive constant on the log scale.  That is, \Stan defines a
function $f(\theta,y,x)$ such that for every legal data vector $y$ and
$x$, there exists a normalizing constant $Z_{y,x}$ depending only on $y$
and $x$ such that
%
\[
\log p(\theta|y;x) = f(\theta,y,x) - \log Z_{y,x},
\]
%
or equivalently
%
\[
p(\theta|y;x) = \exp \! \big( \, f(\theta,y,x) \big)  \, \frac{1}{Z_{y,x}}.
\]
%
\Stan's samplers do not need to compute the normalizing term
$Z_{y,x}$, so it need not be included in the model.  If $Z_{y,x}$
exists, it must satisfy
\[
Z_{y,x} = \int_{\Theta} \, f(\theta,y,x) \, d\theta.
\]

\subsection{Bayesian Inference}

\Stan was developed to enable Bayesian inference through posterior
sampling.  Bayes's rule can be expressed as
%
\[
p(\theta|y;x) 
= \frac{p(y|\theta;x) \, p(\theta;x)}{p(y;x)}
= \frac{p(y|\theta;x) \, p(\theta;x)}
       {\int_{\Theta} p(y|\theta';x) \,  p(\theta';x) \ d\theta'} \, 
\propto p(y|\theta;x) \, p(\theta;x).
\]
%
It shows that the posterior probability function $p(\theta|y;x)$ is
proportional to the product of the sampling probability function
$p(y|\theta;x)$ and the prior probability function $p(\theta;x)$.  The
prior may depend on the unmodeled data elements of $x$, but not on the
modeled data elements in $y$.  The denominator is a normalizing term
which Bayes's rule shows can be reduced to a formula (albeit an
integral) involving only the sampling function and prior.

Bayes's rule therefore allows \Stan to define a model to sample from
the the posterior $p(\theta|y;x)$ using a function defined by
\[
f(\theta,y,x) = \log p(y|\theta;x) + \log p(\theta;x),
\]
in which the implicit normalizing constant reduces to $Z_{y,x} =
p(y;x)$.


\subsection{Pure Simulation}

\Stan programs can be used purely for simulation.  If $y$ is empty,
$\theta$ will consist of simulated values from $p(\theta;x)$.  For
example, if $x = (\mu,\sigma)^{\top}$, $\theta =
(\theta_1,\ldots,\theta_N)$, and 
\[
p(\theta;x) =  \prod_{n=1}^N \distro{Normal}(\theta_n;\mu,\sigma),
\]
then the \Stan program defined by the function
\[
f(\theta;x) = \sum_{n=1}^N \log \distro{Normal}(\theta_n;\mu,\sigma)
\]
will result in $\theta$ being a vector of $N$ samples from
$\distro{Normal}(\mu,\sigma)$.  For more efficient sampling from the
same distribution, the normalizing terms can be dropped and the model
specified with the through the unnormalized log of the normal density,
\[
f(\theta;x) \, = \, -\frac{1}{2} \ \sum_{n=1}^N \left( \frac{\theta_n -
    \mu}{\sigma}\right)^2.
\]


\section{\Stan's Compiler}

\Stan's compiler, \stanc, reads a user program in \Stan's modeling
language and generates a \Cpp class implementing the model specified
by that program.  

\subsection{Variable Transforms}

\Stan automatically applies a multivariate transform (and its Jacobian
determinant) to free any constrained parameters, such as deviations
(constrained to be positive), unit simplexes (a vector constrained to
be positive and to sum to 1), and covariance matrices (symmetric and
positive definite).  From the user's perspective, this 
transform happens behind the scenes, driven by the types declared for 
each of the parameters. 

\subsection{Bayesian Inference}

The generated \Cpp class can be plugged into \Stan's continuous and
discrete samplers to read the data vectors $x$ and $y$ and then draw a
sequence of sample parameter vectors $\theta^{(m)}$ according to the
posterior $p(\theta|y;x)$.  

The resulting samples may be used for full Bayesian inference, much of
which can be carried out within \Stan itself.  For instance,
predictions for unknown future observations may be made directly
within \Stan.  Similarly, fake data simulations from the fitted model
can be generated directly within \Stan and compared to the actual data
for model checking.

\section{Markov Chain Monte Carlo Sampling}

Like \BUGS, \Stan uses Markov chain Monte Carlo (\MCMC) techniques to
generate the samples.  The particular techniques are discussed in the
following this section in \refsection{intro-samplers}.

\MCMC methods, including \Stan's, do not typically draw independent
samples from the posterior $p(\theta|y;x)$.  They are not even
guaranteed to draw samples from the posterior until the chain becomes
stationary, which is only guaranteed in the limit.  This presents two
problems, which can be addressed in practice as follows.

\subsection{Initialization and Convergence Monitoring}

A Markov chain generates (not necessarily independent) samples from
the posterior only after it has converged to a stationary state.
Unfortunately, this is only guaranteed in the limit in theory.  In
practice, diagnostics must be applied to monitor whether the Markov chain(s)
have converged.

Convergence monitoring may be approached several ways, most of which
are hypothesis tests with a null hypothesis that the chains have
converged.  If null hypothesis of stationary samples is rejected by
the test, the chains are unlikely to have converged. 

One commonly applied test is the \cite{GelmanRubin:1992} potential
scale reduction statistic (written $\hat{R}$).  The $\hat{R}$
statistic applies to multiple parallel Markov chains, comparing the
variance of samples within each chain to the variance of the pooled
samples across chains; at convergence these should be the same and
$\hat{R}$ will approach 1.  Gelman and Rubin's recommendation is that
the independent Markov chains be initialized with diffuse starting
values for the parameters.  \Stan allows users to specify initial
values for parameters or can draw diffuse random initializations
itself.

\subsection{Effective Sample Size}

The second issue with \MCMC methods is that the samples tend to be
autocorrelated (i.e., not independent).  This will affect the
estimation of posterior quantities of interest, such as means,
variances or quantiles.

Given a sequence of uncorrelated samples, estimation follows the usual
theory for independent identically distributed (i.i.d.) data.  For
instance, sampling error in the estimate of the mean is easily
characterized as shrinking at a rate of ${\mathcal O}(1/\sqrt{n})$,
where $n$ is the number of samples.

With correlated samples, the number of independent samples in the
denominator, $n$, must be replaced with what is known as the
``effective sample size'' ({\sc ess}).  Because the autocorrelation in
the posterior samples is rarely known analytically, further estimates
must be made of the effective sample size based on the empirical
autocorrelations observed in the samples.  


\section{Stan's Hamiltonian Monte Carlo Samplers}\label{intro-samplers.section}

For continuous variables, \Stan uses Hamiltonian Monte Carlo (\HMC)
sampling. \HMC is a Markov chain Monte Carlo (\MCMC) method based on
simulating the Hamiltonian dynamics of a fictional physical system in
which the parameter vector $\theta$ represents the position of a
particle in $K$-dimensional space and potential energy is defined to
be the negative (unnormalized) log probability.  Each sample in the
Markov chain is generated by starting at the last sample, applying a
random momentum to determine initial kinetic energy, then simulating
the path of the particle in the field.  Standard \HMC runs the
simulation for a fixed number of discrete steps of a fixed step size
and uses a Metropolis adjustment to ensure detailed balance of the
resulting Markovian system.  This adjustment treats the momentum term
of the Hamiltonian as an auxiliary variable, and the only reason for
rejecting a sample will be discretization error in computing the
Hamiltonian.

In addition to standard \HMC, \Stan implements an adaptive
version of \HMC, the No-U-Turn Sampler (\NUTS).  \NUTS automatically
tunes step sizes and a diagonal mass matrix during warmup and then
adapts the number of leapfrog integration steps during sampling.
Stan is expressive enough to allow most discrete variables to be
marginalized out.  For the remaining discrete parameters, \Stan uses
Gibbs sampling if there are only a few outcomes and adaptive slice
sampling otherwise.


\chapter{Getting Started}

This chapter is designed to help users get acquainted with the overall
design of the \Stan language and calling \Stan from the command line.
For installation information, see \refappendix{install}.
Later chapters are devoted to expanding on the material in this
chapter with full reference documentation.


\section{A Minimal Program}

Stan is distributed with several working models.  The simplest of
these is found in the following location relative to the top-level
distribution.
%
\begin{quote}
\begin{Verbatim}
src/models/basic_distributions/normal.stan
\end{Verbatim}
\end{quote}
%
The contents of this file are as follows.
%
\begin{quote}
\begin{Verbatim}
parameters {
  real y;
}
model {
  y ~ normal(0,1);
}
\end{Verbatim}
\end{quote}
%
The model's single parameter \code{y} is declared to take real values.
The probability model specifies that \code{y} has a normal
distribution with location 0 and scale 1.  Basically, this model will
sample a single unit normal variate.  

\section{Whitespace and Semicolons}

In \Stan, every variable declaration and atomic statement must be
terminated by a semicolon (\code{;}).  This is the convention followed
by programming languages such as \Cpp.  It is not the convention
followed by the statistical languages \R, \BUGS, or \JAGS.

The reason for the \Cpp convention is to ensure that differences in
whitespace are not meaningful.  In \R, \BUGS, and \JAGS, the following
is a complete, legal statement.
%
\begin{quote}
\begin{Verbatim}
a <- b +
     c
\end{Verbatim}
\end{quote}
%
In contrast, the usual way of typesetting mathematics and laying out
code in programming languages, with the operator continuing the
expression beginning a new line, is invalid.
%
\begin{quote}
\begin{Verbatim}
a <- b
     + c
\end{Verbatim}
\end{quote}
%
The only difference is in the kind of whitespace between \code{b} and
\code{+} and between \code{+} and \code{c}.  In \Stan, there is no
whitespace-dependent behavior.  Neither of these is a complete
statement, whereas either one terminated with a semicolon is.  The
second form is recommended for \Cpp and \Stan.


\section{Compiling  with {\tt\bfseries stanc}}

Starting at \Stan's home directory, written here as {\tt \$stan},
the model may be compiled by the \Stan compiler, \stanc, into \Cpp code
as follows.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cd $stan
> bin/stanc src/models/basic_distributions/normal.stan
\end{Verbatim}
%
\begin{Verbatim}
Model name=anon_model
Input file=src/models/basic_distributions/normal.stan
Output file=anon_model.cpp
\end{Verbatim}
\end{quote}
%
The output indicates the name of the model, here the default value
\code{anon\_model}, the input file from which the \Stan program is
read, here \code{normal.stan}, and the output file to which the
generated \Cpp code is written, here \code{anon\_model.cpp}.  See
\refchapter{stanc} for more documentation on the \stanc compiler.

\section{Compiling the Generated Code}

The file generated by \stanc must next be compiled with a \Cpp
compiler by linking to \Stan's source and library directories using
the {\tt -I} option of the compiler.  The following example 
uses the \clang compiler for \Cpp.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> g++ -I src -I lib anon_model.cpp 
\end{Verbatim}
\end{quote}
%
This command invokes the \clang compiler for \Cpp to create a
platform-specific executable in the default location, which is {\tt
  a.out} by convention.  If all goes well, as above, there is no
output to the console.  More information about compiling the \Cpp code
generated by \Stan may be found in \refchapter{compiling-cpp}.
Installation information for \Cpp compilers may be found in
\refappendix{install}.

\section{Running the Sampler}

The executable resulting from compiling the generated \Cpp may be run
as follows.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./a.out
\end{Verbatim}
%
\begin{Verbatim}
STAN SAMPLING COMMAND
data = 
init = random initialization
samples = samples.csv
append_samples = 0
seed = 1331941513 (randomly generated)
chain_id=1 (default)
iter = 2000
warmup = 1000
thin = 1
leapfrog_steps = -1
max_treedepth = 10
epsilon = -1
epsilon_pm = 0
epsilon_adapt_off = 0
delta = 0.5
gamma = 0.05

Iteration: 2000 / 2000 [100%]  (Sampling)
\end{Verbatim}
\end{quote}
%
The program indicates to the standard output that the samples are
written to \code{samples.csv}.  The first few lines of this file
are comments about aspects of the run.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cat samples.csv
\end{Verbatim}
\begin{Verbatim}
# Samples Generated by Stan
#
# stan_version_major=alpha
# stan_version_minor=0
# stan_version_patch=0
# data=
# init=random initialization
# append_samples=0
# seed=1331941796
# chain_id=1
# iter=2000
# warmup=1000
# thin=1
# leapfrog_steps=-1
# max_treedepth=10
# epsilon=-1
# epsilon_pm=0
# delta=0.5
# gamma=0.05
...
\end{Verbatim}
\end{quote}
%
The ellipses notation, {\tt ...}, indicates that the output continues
beyond what's shown.  Here, what follows is the data in standard
comma-separate value ({\sc csv}) notation.
%
\begin{quote}
\begin{Verbatim}
...
lp__,treedepth__,y
-0.0126699,1,0.159185
-0.222796,1,-0.667527
-0.222796,1,-0.667527
-0.404457,1,-0.899397
...
\end{Verbatim}
\end{quote}
%
The first line consists of a header indicating the names of the
variables on the lines to follow, and each following line indicates a
single sampled value of the parameters.  The first column is reserved
for the (unnormalized) log probability (density) of the parameters,
with name {\tt lp\_\_} (the underscores are to prevent name conflicts
with user-defined model parameters).  The next values are for
reporting the behavior of the sampler.  In this case, the \NUTS
sampler was used, so there is a report of the depth of tree it
explored, with variable name {\tt treedepth\_\_}.  The remaining
values are parameters.  Here, the model has only one parameter, {\tt
  y}.  The first sampled value for {\tt y} is 0.159185, the second is
-0.667527, and so on.  

Note that the second sampled value is repeated.  This is not a bug.
Rather, it is the behavior to expect from a sampler using a Metropolis
acceptance step for proposals, as \Stan's samplers \HMC and \NUTS do.

\section{Data}

\Stan allows data to be specified in programs, used in models, and
read into compiled \Stan programs. This section provides an example of
coding and running a \Stan program with data stored in a file in the
\SPLUS/\R dump format.

The \Stan program in 
\begin{quote}
\begin{Verbatim}
src/models/basic_estimators/bernoulli.stan
\end{Verbatim}
\end{quote}
can be used to estimate a Bernoulli parameter \code{theta} from
\code{N} binary observations.  The file contains the following code.
%
\begin{quote}
\begin{Verbatim}
data {
  int(0,) N;
  int(0,1) y[N];
}
parameters {
  real(0,1) theta;
}
model {
  theta ~ beta(1,1);
  for (n in 1:N)
    y[n] ~ bernoulli(theta);
}
\end{Verbatim}
\end{quote}
%
This program declares two data variables in its \code{data} block.
The first data variable, \code{N}, is an integer encoding the number
of observations.  The declaration \code{int(0,)} indicates that
\code{N} must take on non-negative values.  The second data variable,
\code{y}, is declared as \code{y[N]}, specifying that it is an array
of \code{N} values.  Each of these values has the declared type,
\code{int(0,1)}, an integer between 0 and 1 inclusive, i.e., a binary
value.  The \code{N} individual binary values in the array \code{y}
are accessed using standard array notation, indexing from 1, as \code{y[1]},
\code{y[2]}, ..., \code{y[N]}.

The \code{parameters} block declares a single parameter, \code{theta}.
Its type is given as \code{real(0,1)}, meaning it takes on continuous
values between 0 and 1 inclusive.  The constraint is necessary in
order to ensure that \code{theta} takes on a legal value as the
success parameter in the Bernoulli distribution in which it is used in
the \code{model} block of the program.

The \code{model} block consists of a for-loop for the data.   The loop is
specified so that the body is executed for values of \code{n} between
\code{1} and \code{N} inclusive.  The body here is a sampling
statement specifying that the variable \code{y[n]} is modeled as
having a Bernoulli distribution with parameter \code{theta}.  

A sample data file for this program can be found in the file
\code{bernoulli.Rdata} in the same directory.  This data file has
the following contents.
%
\begin{quote}
\begin{Verbatim}
N <- 10
y <- c(0,1,0,0,0,0,0,0,0,1)
\end{Verbatim}
\end{quote}
%
A data file must contain appropriate values for all of the data
variables declared in the \Stan program's \code{data} block.  Here there
is a non-negative integer value for \code{N} and an array of length
\code{N} (i.e., 10) integer values between 0 and 1 inclusive.  The
array is coded using the \SPLUS sequence notation \code{c(...)}.
The dump format supported by \Stan is documented in \refchapter{dump}.

The program is compiled by \stanc and the \Cpp compiler in the same
way.  This time, the output model gets an explicitly specified name.
%
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> bin/stanc --name=bern src/models/basic_estimators/bernoulli.stan 
\end{Verbatim}
\begin{Verbatim}
Model name=bern
Input file=src/models/basic_estimators/bernoulli.stan
Output file=bern.cpp
\end{Verbatim}
\end{quote}
%
As before, the \Cpp compiler needs to be given the name of
generated file.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> g++ -O3 -I src -I lib -o bern bern.cpp
\end{Verbatim}
\end{quote}
%
There are two new compiler options here.  The option \code{-O3} sets
optimization to level 3, which generates much faster executable
code at the expense of slower compilation.  The name of the
executable is also specified, using the option \code{-o~bern}.  Now
the code may be executed by calling its executable with the data file
specified. 
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./bern --data=src/models/basic_estimators/bernoulli.Rdata
\end{Verbatim}
\end{quote}

\section{Proper and Improper Priors}

The model in the previous section does not contain a sampling
statement for \code{theta}.  The default behavior is to give
\code{theta} a uniform prior.  In this case, a uniform prior is proper
because \code{theta} is bounded to a finite interval.  Improper priors
are also allowed in \Stan programs; they arise from unconstrained
parameters without sampling statements.  The uniform prior could have
also been added explicitly by adding the following statement to the
\code{model} block of the program.
%
\begin{quote}
\begin{Verbatim} 
theta ~ uniform(0,1);
\end{Verbatim}
\end{quote}
% 
A third way to specify that \code{theta} has a uniform distribution
between 0 and 1 is with the beta distribution.
%
\begin{quote}
\begin{Verbatim}
theta ~ beta(1,1);
\end{Verbatim}
\end{quote}
%
The beta distribution is conjugate to the Bernoulli, but \Stan (at
least as of yet) does not make use of this information.  On the other hand,
these three approaches, no prior, uniform prior, and beta prior,
are equally efficient in \Stan's sampler, because their uniformity
can be determined at compile time and thus computations related to
them eliminated.  There is further discussion of \Stan optimization
in \refchapter{optimization}.

\section{Comments}

\Stan supports the three major style of comments.  

If the character \code{\#} appears on a line, that character and every
character up to but not including the end of line is ignored.  This is
the style of comments used in Python, R, and the shell.  

If the character pair \code{//} appears on a line, that pair and every
character up to but not including the end of line is ignored.  This is
the style of comments used in C.  This is the preferred commenting
style for line-based comments and for commenting out code.

\Stan also supports \Cpp comment style in which any content placed
between \code{/*} and \code{*/} is ignored along with the boundary
markers.  This is the preferred style for long documentation comments.
It is difficult to use this method to comment out code, because 
an internal close comment sequence, \code{*/}, may take precedence.


\section{User-Defined Distributions and Functions}

\Stan allows new distributions to be coded directly in its modeling
language, as described in \refchapter{custom-probability-functions}. 

Extending \Stan's underlying \Cpp API provides a way to add new
functions that may be implemented efficiently and used across
different \Stan programs.

\refappendix{user-defined-functions} documents the way in which new
basic functions may be added to \Stan.  The new functions may use
algorithmic differentiation to compute the partial derivatives of
their output with respect to the input variables, or the user may
directly specify the partials for increased efficiency.

