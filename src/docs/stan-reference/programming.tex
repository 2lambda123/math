





\part{Programming Techniques}\label{programming-techniques.part}



\chapter{Missing Data}

\section{Known and Unknown Quantities}

\BUGS and \R support mixed arrays of known and missing data.  In
\BUGS, known and unknown values may be mixed as long as every unknown
variable appears on the left-hand side of either an assignment or
sampling statement.  

\Stan treats variables declared in the \code{data} and
\code{transformed data} blocks as known and the variables in the
\code{parameters} block as unknown.

\subsection{Mixing Known and Unknown Quantities}

The \code{transformed parameters} block may be used to combine known
and unknown values.  The following program skeleton provides an example.
%
\begin{quote}
\begin{Verbatim}
data {
  int K;   
  int N_obs;               
  int N_miss;
  real y_obs[N_obs];
}
transformed data {
  int N;   
  N <- N_obs + N_miss;
}
parameters {
  real beta[K];
  real(0,) sigma;
  real y_miss[N_miss];
}
transformed parameters {
  real y[N];
  for (n in 1:N_obs)  
    y[n] <- y_obs[n];
  for (n in 1:N_miss) 
      y[n + N_obs] <- y_miss[n];
}
model {
  for (n in 1:N)
    y[n] ~ normal(mu,sigma);
}
\end{Verbatim}
\end{quote}
%
The transformed parameter array \code{y} is declared to be of size
\code{N}, the combined size of the observed and missing data arrays,
and then defined by copying the observed data array \code{y\_obs}
followed by the missing parameter array \code{y\_miss}.  In the model
block, the resulting mixed array \code{y} may be used like any other
variable.

Running the resulting program will sample values for the the missing
data \code{y\_miss} just like the other parameters \code{mu} and
\code{sigma}.

\subsection{Missing Data}

Although useful in some situations, creating a mixed transformed
parameter array is not the recommended way to program missing data
problems in \Stan.  The behavior is correct, but the computation is
wasteful.  Each parameter, be it declared in the \code{parameters} or
\code{transformed parameters} block, uses an algorithmic
differentiation variable which is more expensive in terms of memory
and gradient-calculation time than a simple data variable.
Furthermore, the copy takes up extra space, and the copy takes extra
time.

The recommended approach keeps the same \code{data} and
\code{parameters} blocks to define the observed and missing data.
%
\begin{quote}
\begin{Verbatim}
data {
  int N_obs;
  int N_miss;
  real y_obs[N_obs];
}
parameters {
  real mu;
  real(0,) sigma;
  real y_miss[N_miss];
}
model {
  for (n in 1:N_obs)
    y_obs[n] ~ normal(mu,sigma);
  for (n in 1:N_miss)
    y_miss[n] ~ normal(mu,sigma);
}
\end{Verbatim}
\end{quote}
%
There is no longer any transformed data.  Instead, the model contains
one loop over the observed data and one over the missing data.  This
model will more efficiently compute the same samples over the
distribution parameters and the missing data (it differs slightly in
not also providing the combined array \code{y} of transformed parameters).




\chapter{Truncated or Censored Data}



\chapter{Mixture Modeling}

Finite mixture distributions can be coded in \Stan as custom
distributions combining the mixture with the built-in probability
functions.  

In the simplest case, consider a random variable $Y$ whose
distirbution the mixture of two normal distributions, with a 0.3
weight on $\distro{Normal}(-1,2)$ and a 0.7 weight on
$\distro{Normal}(3,1)$.  Mathematically, the densities are mixed, with
the density of $Y$ given by
\[
p_Y(y) = 0.3 \times \distro{Normal}(y|-1,2) \, + \, 0.7 \times \distro{Normal}(y|3,1).
\]
%
The \Stan model with parameter $Y$ is coded as follows.
%
\begin{quote}
\begin{Verbatim}
parameters {
  real y;
}
model {
   lp__ <- log_sum_exp(log(0.3) + normal_log(y,-1,2),
                       log(0.7) + normal_log(y,3,1));
}
\end{Verbatim}
\end{quote}
%
The log probability term is derived by taking
\begin{eqnarray*}
\log p_Y(y) & = & \log \, \left( 0.3 \times \distro{Normal}(y|-1,2) \, + \,
  0.7 \times
  \distro{Normal}(y|3,1) \, \right)
\\[2pt]
& = & \log(\! \begin{array}[t]{l}
                 \exp(\log(0.3 \times \distro{Normal}(y|-1,2))) \\
                 + \exp(\log(0.7 \times \distro{Normal}(y|3,1))) \ )
              \end{array}
% \\[4pt]
% & = & \log( \! \begin{array}[t]{l}\exp(\log(0.3) + \log \distro{Normal}(y|-1,2))
%             \\
%            + \exp(\log(0.7) + \log \distro{Normal}(y|3,1)) \ )
%             \end{array}
\\[2pt]
& = & \mbox{logSumExp}(\! \begin{array}[t]{l}
                         \log(0.3) + \log \distro{Normal}(y|-1,2),
                         \\                  
                         \log(0.7) + \log \distro{Normal}(y|3,1) \ ).
                       \end{array}
\end{eqnarray*}

Given the scheme for representing mixtures, it may be moved to an
estimation setting, where the locations, scales, and mixture
components are unknown.  Further generalizing to an arbitrary number
of mixture components yields the following model.
%
\begin{quote}
\begin{Verbatim}
data {
  int(1,) K;           // number of mixture components
  int(1,) N;           // number of data points
  real y[N];           // observations
}
parameters {
  simplex(K) theta;    // mixing proportions
  real mu[K];          // locations of mixture components
  real(0,) sigma[K];   // scales of mixture components
}
model {
  real ps[K];          // temp for log component densities
  for (k in 1:K) {
    mu[k] ~ normal(0,10);
    sigma[k] ~ uniform(0,10);
  }
  for (n in 1:N) {
    for (k in 1:K) {
      ps[k] <- log(theta[k]) 
               + normal_log(y[n],mu[k],sigma[k]);
    }
    lp__ <- lp__ + log_sum_exp(ps);    
  }
}
\end{Verbatim}
\end{quote}
%
The model involves \code{K} mixture components and \code{N} data
points.  The mixing proportions are defined to be a unit $K$-simplex
using \code{simplex(K)}, the components distributions locations
\code{mu[k]} are unconstrained and their scales \code{sigma[k]}
constrained to be positive.  The model declares a local variable
\code{ps} of type \code{real[K]}, which is used to accumulate the
contributions by each mixture component.

The locations and scales are drawn from simple priors for the sake of
this example, but could be anything supported by \Stan in general,
including a hierarchical model.  

The main action is in the loop over data points \code{n}.  For each
such point, the log of $\theta_k \times
\distro{Normal}(y_n|\mu_k,\sigma_k)$ is calculated and added to the
array \code{ps}.  Then the log probability is incremented with the log
sum of exponentials of those values.

\chapter{Summing Out Discrete Parameters}



\chapter{Order Constraints}




\chapter{Custom Probability  Functions}%
\label{custom-probability-functions.chapter}


Custom distributions may also be implemented directly within \Stan's
programming language.  A simple example is the triangle distribution.
If $\alpha \in \reals$ and $\beta \in (\alpha,\infty)$, then
$y \in (\alpha,\beta)$ has a density defined as follows.
\[
\distro{Triangle}(y | \alpha,\beta)
= 
\frac{2}{\beta - \alpha}
\
\left(
1 - 
\left|
y - \frac{\alpha + \beta}{\beta - \alpha}
\right|
\right)
\]
%
If $\alpha = -1$, $\beta = 1$, and $y \in (-1,1)$, this reduces to
\[
\distro{Triangle}(y,-1,1) = 1 - |y|.
\]
The file \url{src/models/basic_distributions/triangle.stan} contains
the following \Stan implementation of a sampler from 
$\distro{Triangle}(-1,1)$.
%
\begin{quote}
\begin{Verbatim}
parameters {
    real(-1,1) y;
}
model {
    lp__ <- lp__ + log1m(fabs(y));
}
\end{Verbatim}
\end{quote}
%
The single scalar parameter \code{y} is declared as lying in the
interval \code{(-1,1)}.  The log probability variable \code{lp\_\_} is
incremented with the joint log probabilty of all parameters, i.e.,
$\log \distro{Triangle}(y|-1,1)$.  This value is coded in \Stan as
\code{log1m(fabs(y))}.  The function \code{log1m} is is defined so
that \code{log1m(x)} has the same value as \code{log(1.0-x)}, but the
computation is faster, more accurate, and more stable.

The log probability variable \code{lp\_\_} is incremented in this
program rather than being set.  This is because the transform involved
for the bounded variable \code{y} of type \code{real(-1,1)} implicitly
adds a term to \code{lp\_\_} to adjust the log probability for the
transform (adding the log absolute derivative of the inverse
transform).

The constrained type \code{real(-1,1)} declared for \code{y} is
critical for correct sampling behavior.  If the constraint on \code{y}
is removed from the program, say by declaring \code{y} as having the
unconstrained scalar type \code{real}, the program would compile, but
it would produce arithmetic exceptions at run time when the sampler
explored values of \code{y} outside of $(-1,1)$.

Now suppose the log probability function were extended to all of
$\reals$ as follows by defining the probability to be \code{log(0.0)},
i.e., $-\infty$, for values outside of $(-1,1)$.
%
\begin{quote}
\begin{Verbatim}
    lp__ <- log(fmax(0.0,1 - fabs(y)));
\end{Verbatim}
\end{quote}
%
With the constraint on \code{y} in place, this is just a less
efficient, slower, and less arithmetically stable version of the
original program.  But if the constraint on \code{y} is removed, 
the model will compile and run without arithmetic errors, but will not
sample properly.%
%
\footnote{The problem is the (extremely!) light tails of the triangle
  distribution.  The standard \HMC and \NUTS samplers cannot get into the
  corners of the triangle properly.  Because the actual code declares
  \code{y} to be of type \code{real(-1,1)}, the inverse logit
  transform is applied to the unconstrained variable and its log
  absolute derivative added to the log probability.  The resulting
  distribution on the logit-transformed \code{y} is well behaved.  See
  \refchapter{variable-transforms} for more information on the actual
  transforms used.}


