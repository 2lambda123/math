





\part{Programming Techniques}\label{programming-techniques.part}



\chapter{Missing Data}

\section{Known and Unknown Quantities}

\BUGS and \R support mixed arrays of known and missing data.  In
\BUGS, known and unknown values may be mixed as long as every unknown
variable appears on the left-hand side of either an assignment or
sampling statement.  

\Stan treats variables declared in the \code{data} and
\code{transformed data} blocks as known and the variables in the
\code{parameters} block as unknown.

\subsection{Mixing Known and Unknown Quantities}

The \code{transformed parameters} block may be used to combine known
and unknown values.  The following program skeleton provides an example.
%
\begin{quote}
\begin{Verbatim}
data {
  int K;   
  int N_obs;               
  int N_miss;
  real y_obs[N_obs];
}
transformed data {
  int N;   
  N <- N_obs + N_miss;
}
parameters {
  real beta[K];
  real(0,) sigma;
  real y_miss[N_miss];
}
transformed parameters {
  real y[N];
  for (n in 1:N_obs)  
    y[n] <- y_obs[n];
  for (n in 1:N_miss) 
      y[n + N_obs] <- y_miss[n];
}
model {
  for (n in 1:N)
    y[n] ~ normal(mu,sigma);
}
\end{Verbatim}
\end{quote}
%
The transformed parameter array \code{y} is declared to be of size
\code{N}, the combined size of the observed and missing data arrays,
and then defined by copying the observed data array \code{y\_obs}
followed by the missing parameter array \code{y\_miss}.  In the model
block, the resulting mixed array \code{y} may be used like any other
variable.

Running the resulting program will sample values for the the missing
data \code{y\_miss} just like the other parameters \code{mu} and
\code{sigma}.

\subsection{Missing Data}

Although useful in some situations, creating a mixed transformed
parameter array is not the recommended way to program missing data
problems in \Stan.  The behavior is correct, but the computation is
wasteful.  Each parameter, be it declared in the \code{parameters} or
\code{transformed parameters} block, uses an algorithmic
differentiation variable which is more expensive in terms of memory
and gradient-calculation time than a simple data variable.
Furthermore, the copy takes up extra space, and the copy takes extra
time.

The recommended approach keeps the same \code{data} and
\code{parameters} blocks to define the observed and missing data.
%
\begin{quote}
\begin{Verbatim}
data {
  int N_obs;
  int N_miss;
  real y_obs[N_obs];
}
parameters {
  real mu;
  real(0,) sigma;
  real y_miss[N_miss];
}
model {
  for (n in 1:N_obs)
    y_obs[n] ~ normal(mu,sigma);
  for (n in 1:N_miss)
    y_miss[n] ~ normal(mu,sigma);
}
\end{Verbatim}
\end{quote}
%
There is no longer any transformed data.  Instead, the model contains
one loop over the observed data and one over the missing data.  This
model will more efficiently compute the same samples over the
distribution parameters and the missing data (it differs slightly in
not also providing the combined array \code{y} of transformed parameters).




\chapter{Truncated or Censored Data}

Data in which measurements have been truncated or censored can be
coded in \Stan following their respective probability models.

\section{Truncated Data}

Truncated data is data for which measurements are only reported if
they fall above a lower bound, below an upper bound, or between a
lower and upper bound.  

Truncated data may be modeled in \Stan using truncated distributions.
For example, suppose the truncated data is $y_n$ with an upper
truncation point of $U = 300$ so that $y_n < 300$.  In \Stan, this
data can be modeled as following a truncated normal distribution for
the observations as follows. 
%
\begin{quote}
\begin{Verbatim} 
data {
   int(0,) N;
   real U;
   real(,U) y[N];
} 
parameters
   real mu;
   real(0,) sigma;
} 
model {
  for (n in 1:N)
       y[n] ~ normal(mu,sigma) T(,U);
}
\end{Verbatim}
\end{quote}
% 
The model declares an upper bound \code{U} as data and constrains
the data for \code{y} to respect the constraint;  this will be checked
when the data is loaded into the model before sampling begins.

This model implicitly uses an improper flat prior on the scale and
location parameters; these could be given priors in the model.

\subsection{Unknown Truncation Points}

Nothing prevents the truncation points from being estimated as
parameters.  This can be done with a slight rearrangement of the
variable declarations from the model for known truncation points.
%
\begin{quote}
\begin{Verbatim}
data {
   int(1,) N;
   real y[N];
}
parameters
   real(,min(y)) L; 
   real(max(y),) U;
   real mu;
   real(0,) sigma;
}
model {
  L ~ ...;  
  U ~ ...;
  for (n in 1:N)
       y[n] ~ normal(mu,sigma) T(L,U);
}
\end{Verbatim}
\end{quote}
%
Here there is a lower truncation point \code{L} which is declared to
be less than or equal to the minimum value of \code{y}.  The upper
truncation point \code{U} is declared to be larger than the maximum
value of \code{y}.  This declaration, although dependent on the data,
only enforces the constraint that the data fall within the
truncation bounds.  With the constraint that there is at least one
data point \code{y[n]}, the constraint that \code{L} is less than
\code{U} is enforced indirectly.

The ellipses where the priors for the bounds \code{L} and \code{U}
should go should be filled in with a an informative prior in
order for this model to not concentrate \code{L} strongly around 
\code{min(y)} and \code{U} strongly around \code{max(y)}.


\section{Censored Data}

Censoring hides values from points that are too large, too small, or
both.  Unlike with truncated data, the number of data points that were
censored is known.  The textbook example is the household scale which
does not report values above 300 pounds.  

\subsection{Imputing Censored Values}

One way to model censored data is to treat the censored data as
missing data that is constrained to fall in the censored range of
values.  Because \Stan does not allow unknown values in its arrays or
matrices, the censored values must be represented explicitly.
%
\begin{quote}
\begin{Verbatim}
data {
   int(0,) N_obs;
   int(0,) N_cens;
   real(0,) y_obs[N_obs];
   real(max(y_obs),) U;
}
parameters {
  real(U,) y[N_cens];
  real mu;
  real(0,) sigma;
}
model {
   for (n in 1:N_obs)
       y_obs[n] ~ normal(mu,sigma);
   for (y in 1:N_cens)
       y_cens[n] ~ normal(mu,sigma);
}
\end{Verbatim}
\end{quote}
%
Because the censored data array \code{y} is declared to be a parameter, it
will be sampled along with the location and scale parameters \code{mu}
and \code{sigma}.  Because the censored data array \code{y} is
declared to have values of type \code{real(U,)}, all imputed values
for censored data will be greater than \code{U}.  The imputed censored
data affects the location and scale parameters through the last
sampling statement in the model.  

\subsection{Integrating out Censored Values}

Although it is wrong to ignore the censored values in estimating
location and scale, it is not necessary to impute values.  Instead,
the values can be integrated out.  Each censored data point has a
probability of
%
\[
\mbox{Pr}[y > U] 
= \int_U^{\infty} \distro{Normal}(y|\mu,\sigma) \, dy
= 1 - \Phi\left(\frac{y - \mu}{\sigma}\right).
\]
%
With $M$ censored observations, the total probability on the log scale
is
\[
\log \prod_{m=1}^M \mbox{Pr}[y_m > U]
= \log \left( 1 - \Phi\left(\frac{y - \mu}{\sigma}\right)\right)^{M}
= M \, \log \left( 1 - \Phi\left(\frac{y - \mu}{\sigma}\right)\right)
\]

% 
This can be implemented directly in \Stan using the cumulative normal
distribution function \code{normal\_p} as follows.
%
\begin{quote}
\begin{Verbatim}
data {
   int(0,) N_obs;
   int(0,) N_cens;
   real(0,) y_obs[N_obs];
   real(max(y_obs),) U;
}
parameters {
  real mu;
  real(0,) sigma;
}
model {
   for (n in 1:N_obs)
       y_obs[n] ~ normal(mu,sigma);
   lp__ <- lp__ + N_cens * log1m(normal_p(U,mu,sigma));
}
\end{Verbatim}
\end{quote}








\chapter{Mixture Modeling}

Mixture models of an outcome assume that the outcome is drawn from one
of several distributions, the identity of which is controlled by a
categorical mixing distribution.  Mixture models typically have
multimodal densities with modes around the modes of the mixture
components.  

\section{Latent Discrete Parameterization}

One way to parameterize a mixture model is with a latent categorical
variable indicating which mixture component was responsible for the
outcome. For example, consider $K$ normal distributions with locations
$\mu_k \in \reals$ and scales $\sigma_k \in (0,\infty)$ mixed in the
proportions $\theta$ in the $K$-simplex.  For each outcome $y_n$ there
is a latent variable $z_n$ in $\setlist{1,\ldots,K}$,
%
\[
z_n \sim \distro{Categorical}(\theta).
\]
%
The variable $y_n$ is distributed according to the parameters
of the mixture component $z_n$, 
\[
y_n \sim \distro{Normal}(\mu_{z[n]},\sigma_{z[n]}).
\]
%
This model is not directly supported by \Stan because it involves a
discrete parameter $z$.  

\section{Summing out the Responsibility Parameter}

To implement the normal mixture model outlined in the previous
section in \Stan, the discrete parameters can be summed out of the
model. If $Y$ is a mixture of $K$ normal distributions with 
locations $\mu_k$ and scales $\sigma_k$ with mixing proportions
$\theta$ in the $K$-simplex, then 
\[
p_Y(y) = \sum_{k=1}^K \distro{Normal}(\mu_k,\sigma_k).
\]

For example, the mixture of $\code{Normal}(-1,2)$ and
$\code{Normal}(3,1)$ with mixing proportion $\theta =
(0.3,0.7)^{\top}$ can be implemented in \Stan as follows.
%
\begin{quote}
\begin{Verbatim}
parameters {
  real y;
}
model {
   lp__ <- log_sum_exp(log(0.3) + normal_log(y,-1,2),
                       log(0.7) + normal_log(y,3,1));
}
\end{Verbatim}
\end{quote}
%
The log probability term is derived by taking
\begin{eqnarray*}
\log p_Y(y) & = & \log \, \left( 0.3 \times \distro{Normal}(y|-1,2) \, + \,
  0.7 \times
  \distro{Normal}(y|3,1) \, \right)
\\[2pt]
& = & \log(\! \begin{array}[t]{l}
                 \exp(\log(0.3 \times \distro{Normal}(y|-1,2))) \\
                 + \exp(\log(0.7 \times \distro{Normal}(y|3,1))) \ )
              \end{array}
% \\[4pt]
% & = & \log( \! \begin{array}[t]{l}\exp(\log(0.3) + \log \distro{Normal}(y|-1,2))
%             \\
%            + \exp(\log(0.7) + \log \distro{Normal}(y|3,1)) \ )
%             \end{array}
\\[2pt]
& = & \mbox{logSumExp}(\! \begin{array}[t]{l}
                         \log(0.3) + \log \distro{Normal}(y|-1,2),
                         \\                  
                         \log(0.7) + \log \distro{Normal}(y|3,1) \ ).
                       \end{array}
\end{eqnarray*}

Given the scheme for representing mixtures, it may be moved to an
estimation setting, where the locations, scales, and mixture
components are unknown.  Further generalizing to an arbitrary number
of mixture components yields the following model.
%
\begin{quote}
\begin{Verbatim}
data {
  int(1,) K;           // number of mixture components
  int(1,) N;           // number of data points
  real y[N];           // observations
}
parameters {
  simplex(K) theta;    // mixing proportions
  real mu[K];          // locations of mixture components
  real(0,) sigma[K];   // scales of mixture components
}
model {
  real ps[K];          // temp for log component densities
  for (k in 1:K) {
    mu[k] ~ normal(0,10);
    sigma[k] ~ uniform(0,10);
  }
  for (n in 1:N) {
    for (k in 1:K) {
      ps[k] <- log(theta[k]) 
               + normal_log(y[n],mu[k],sigma[k]);
    }
    lp__ <- lp__ + log_sum_exp(ps);    
  }
}
\end{Verbatim}
\end{quote}
%
The model involves \code{K} mixture components and \code{N} data
points.  The mixing proportions are defined to be a unit $K$-simplex
using \code{simplex(K)}, the components distributions locations
\code{mu[k]} are unconstrained and their scales \code{sigma[k]}
constrained to be positive.  The model declares a local variable
\code{ps} of type \code{real[K]}, which is used to accumulate the
contributions by each mixture component.

The locations and scales are drawn from simple priors for the sake of
this example, but could be anything supported by \Stan in general,
including a hierarchical model.  

The main action is in the loop over data points \code{n}.  For each
such point, the log of $\theta_k \times
\distro{Normal}(y_n|\mu_k,\sigma_k)$ is calculated and added to the
array \code{ps}.  Then the log probability is incremented with the log
sum of exponentials of those values.





\chapter{Regression Models}

\Stan supports the full range of regression models from simple linear
regressions to multilevel generalized linear models.  Coding
regression models in \Stan is very much like coding them in \BUGS.

\section{Linear Regression}

The simplest linear regression model is the following, with a single
predictor and intercept term.

\begin{quote}
\begin{Verbatim}
data {
   int(0,) N;
   real x[N];
   real y[N];
}
parameters {
    real alpha;
    real beta;
    real(0,) epsilon;
}
model {
    for (n in 1:N)
        y[n] ~ normal(beta * x[n] + alpha, epsilon);
}
\end{Verbatim}
\end{quote}
%
This model has improper priors for the two regression coefficients
(intercept \code{alpha} and slope \code{beta}) and the noise term.

\section{Coefficient and Noise Priors}

There are several ways in which this model can be generalized.  
For example, weak priors can be assigned to the coefficients as follows.
%
\begin{quote}
\begin{Verbatim}
    alpha ~ normal(0,100);
    beta ~ normal(0,100);
    epsilon ~ uniform(0,100);
\end{Verbatim}
\end{quote}
%
More informative priors based the (half) Cauchy distribution are coded
as follows.
%
\begin{quote}
\begin{Verbatim}
    alpha ~ cauchy(0,2.5);
    beta ~ cauchy(0,2.5);
    epsilon ~ cauchy(0,2.5);
\end{Verbatim}
\end{quote}
%
The regression coefficients \code{alpha} and \code{beta} are
unconstrained, but \code{epsilon} must be positive and properly
requires the half-Cauchy distribution, which would be expressed in
\Stan using truncation as follows.
%
\begin{quote}
\begin{Verbatim}
    epsilon ~ cauchy(0,2.5) T(0,);
\end{Verbatim}
\end{quote}
%
Because the log probability function need not be normalized, the extra
factor of 2 is not needed in the model, so the simpler un-truncated
distribution suffices.

\section{Robust Noise Models}

The standard approach to linear regression is to model the noise
term $\epsilon$ as having a normal distribution.  From \Stan's
perspective, there is nothing special about normally distributed
noise.  For instance, robust regression can be accomodated by giving
the noise term a Student-$t$ distribution.  To code this in \Stan, the
sampling distribution is changed, as follows.
%
\begin{quote}
\begin{Verbatim}
data {
    ...
    real(0,) nu;
}
...
model {
    for (n in 1:N)
        y[n] ~ student_t(nu,0,sigma);
}
\end{Verbatim}
\end{quote}
%
The degrees of freedom constant \code{nu} is specified as data.
Alternatively, it could be hard coded into the model.

\section{Logistic and Probit Regression}

For binary outcomes, logistic or probit regression can be used.  A
logistic regression with one predictor and an intercept is coded as
follows. 
%
\begin{quote}
\begin{Verbatim}
data {
    int(0,) N;
    real x[N];
    int(0,1) y[N];
}
parameters {
    real alpha;
    real beta;
}
model {
    for (n in 1:N)
        y[n] ~ bernoulli(inv_logit(beta * x[n] + alpha));
} 
\end{Verbatim}
\end{quote}
%
The noise parameter is built into the Bernoulli here.  

Logistic regression is a kind of generalized linear model with binary
outcomes and the log odds (logit) link function.  The inverse of the
link function appears in the model.  

Other link functions may be used in the same way.  For example, probit
regression uses the cumulative normal distribution function, which is
typically written as 
\[
\Phi(x) = \int_{-\infty}^x \distro{Normal}(y|0,1) \, dy.
\]
%
The cumulative unit normal distribution function $\Phi$ is also known
as the error function and written as \code{erf(x)}.  The probit
regression model may be coded in \Stan as follows.
%
\begin{quote}
\begin{Verbatim}
        y[n] ~ bernoulli(erf(beta * x[n] + alpha));
\end{Verbatim}
\end{quote}

\section{Multi-Logit Regression}

Multiple outcome forms of logistic regression can be coded directly in
\Stan.  For instance, suppose there are $K$ possible outcomes for each
output variable $y_n$.  Also suppose that there is a $D$-dimensional
vector $x_n$ of predictors for $y_n$.  The multi-logit model with
improper flat priors on the coefficients is coded as follows.
%
\begin{quote}
\begin{Verbatim}
data {
   int(0,) N;
   int(1,K) y[N];
   row_vector(K) x[N];
}
parameters {
   matrix(N,K) beta;
}
model {
   for (n in 1:N)
       y[n] ~ categorical(softmax(x[n] * beta));
}
\end{Verbatim}
\end{quote}
%

\section{Ordered Logistic Regression}\label{ordered-logistic.section}

Ordered logistic regression for an outcome $y_n \in
\setlist{1,\ldots,K}$ with predictors $x_n \in \reals^D$ is determined
by a single coefficient vector $\beta \in \reals^D$ along with a
sequence of cutpoints $c \in \reals^{D-1}$ sorted so that $c_d <
c_{d+1}$.  The discrete output is $k$ if the linear predictor $x_n
\beta$ falls between $c_{k-1}$ and $c_k$, assuming $c_0 = -\infty$ and
$c_K = \infty$.

The ordinal logistic model can be coded in \Stan using the
\code{ordered} data type for the cutpoints and the built-in
\code{ordered\_logistic} distribution.
%
\begin{quote}
\begin{Verbatim} 
data {
  int(1,) D;
  int(2,) K;
  int(0,) N;
  int(1,K) y[N];
  row_vector(D) x[N];
} 
parameters {
  matrix(D) beta;
  ordered(K-1) c;
} 
model {
  for (n in 1:N)
      y[n] ~ ordered_logistic(x[n] * beta, c);
}
\end{Verbatim}
\end{quote}
% 
The vector of cutpoints \code{c} is declared as \code{ordered(D-1)},
which guarantees that \code{c[k]} is less than \code{c[k+1]}.

An ordered probit model could be coded in a manner similar to the
\BUGS encoding of an ordered logistic model.
%
\begin{quote}
\begin{Verbatim}
data {
  int(1,) D;
  int(2,) K;
  int(0,) N;
  int(1,K) y[N];
  row_vector(D) x[N];
}
parameters {
  matrix(D) beta;
  ordered(K-1) c;
}
model {
  vector(K) theta;
  for (n in 1:N) {
      real eta;
      eta <- x[n] * beta;
      theta[1] = 1 - erf(eta - c(1));
      for (k in 2:(K-1))
          theta[k] = erf(eta - c(k-1)) - erf(eta - c(k));
      theta[K] = erf(eta - c(K-1));
      y[n] ~ categorical(theta);
  }
}
\end{Verbatim}
\end{quote}
%
The logistic model could be coded this way by replacing \code{erf}
with \code{inv\_logit}, though the built-in encoding is more efficient
and more numerically stable.  A small efficiency gain could be
achieved by computing the values \code{1 - erf(eta - c(d))} once and
storing them for re-use.

\section{Hierarchical Logistic Regression}

The simplest multilevel model is a hierarchical model in which the
data is grouped into $L$ levels.  An extreme approach would be to
completely pool all the data and estimate a common vector of
regression coefficients $\beta$.  At the other extreme, an approach
would no pooling assigns each level $l$ its own coefficient vector
$\beta_l$ that is estimated separately from the other levels.  A
hierarchical model is an intermediate solution where the degree of
pooling is determined by the data and a prior on the amount of
pooling.

Suppose each binary outcome $y_n \in \setlist{0,1}$ has an associated
level, $ll_n \in \setlist{1,\ldots,L}$.  Each outcome will also have
an associated predictor vector $x_n \in \reals^D$.  Each level $l$
gets its own coefficient vector $\beta_l \in \reals^D$.  The
hierarchical structure involves drawing the coefficients $\beta_{l,d}
\in \reals$ from a prior that is also estimated with the data.  This
hierarchically estimated prior determines the amount of pooling; if
the data in each level are very similar, the pooling will be strong,
but if it is very dissimilar, the pooling will be weak.

The following model encodes a hierarchical logistic regression model
with a hierarchical prior on the regression coefficients.
%
\begin{quote}
\begin{Verbatim}
data {
    int(1,) D;
    int(0,) N;
    int(1,) L;
    int(0,1) y[N];
    int(1,L) ll[N];
}
parameters {
    real mu[D];
    real(0,) sigma[D];
    vector(D) beta[L];
}
model {
    for (d in 1:D) {
        mu[d] ~ normal(0,100);
        sigma[d] ~ uniform(0,1000);
        for (l in 1:L)
            beta[l,d] ~ normal(mu[d],sigma[d]);
    }
    for (n in 1:N)
        y[n] ~ bernoulli(inv_logit(x[n] * beta[ll[n]]));
}
\end{Verbatim}
\end{quote}   


\section{Autoregressive Models}

A first-order autoregressive model (AR(1)) with normal noise takes
each point $y_n$ in a sequence $y$ to be generated according to
%
\[
y_n \sim \distro{Normal}(\alpha + \beta y_{n-1}, \sigma).
\]
%
That is, the expected value of $y_n$ is $\alpha + \beta y_{n-1}$, with
noise scaled as $\sigma$.

\subsection{AR(1) Models}

With improper flat priors on the regression coefficients for slope
($\beta$), intercept ($\alpha$), and noise scale ($\sigma$),
the \Stan program for the AR(1) model is as follows.
%
\begin{quote}
\begin{Verbatim}
data {
    int(0,) N;
    real y[N];
}
parameters {
    real alpha;
    real beta;
    real sigma;
}
model {
    for (n in 2:N)
       y[n] ~ normal(alpha + beta * y[n-1], sigma);
}
\end{Verbatim}
\end{quote}
%

\subsection{Extensions to the Model} 

Proper priors of a range of different families may be added for the
regression coefficients and noise scale.  The normal noise model can
be changed to a Student-$t$ distribution or any other distribution
with unbounded support.  The model could also be made hierarchical if
multiple series of observations are available.  

To enforce the estimation of a stationary AR(1) process, the slope
coefficient \code{beta} may be constrained with bounds as follows.
%
\begin{quote}
\begin{Verbatim}
    real(-1,1) beta;
\end{Verbatim}
\end{quote}
%
In practice, such a constraint is not recommended.  If the data is not
stationary, it is best to discover this while fitting the model.
Stationary parameter estimates can be encouraged with a prior favoring
values of \code{beta} near zero.


\subsection{AR(2) Models}

Extending the order of the model is also straightforward.  For
example, an AR(2) model could be coded with the second-order
coefficient \code{gamma} and the following model statement.
%
\begin{quote}
\begin{Verbatim}
    for (n in 3:N)
       y[n] ~ normal(alpha + beta * y[n-1] + gamma * y[n-2], sigma);
\end{Verbatim}
\end{quote}


\subsection{AR($K$) Models}

A general model where the order is itself given as data can be coded
by putting the coefficients in an array and computing the linear
predictor in a loop.
%
\begin{quote}
\begin{Verbatim}
data {
    int(0,) K;
    ...
parameters {
    real beta[K];
    ...
model {
    for (n in (K+1):N)
        real mu;
        mu <- alpha;
        for (k in 1:K)
            mu <- mu + beta[k] * x[n-k];
        y[n] ~ normal(mu,sigma);
    }
}
\end{Verbatim}
\end{quote}





\chapter{Custom Probability  Functions}%
\label{custom-probability-functions.chapter}


Custom distributions may also be implemented directly within \Stan's
programming language.  A simple example is the triangle distribution.
If $\alpha \in \reals$ and $\beta \in (\alpha,\infty)$, then
$y \in (\alpha,\beta)$ has a density defined as follows.
\[
\distro{Triangle}(y | \alpha,\beta)
= 
\frac{2}{\beta - \alpha}
\
\left(
1 - 
\left|
y - \frac{\alpha + \beta}{\beta - \alpha}
\right|
\right)
\]
%
If $\alpha = -1$, $\beta = 1$, and $y \in (-1,1)$, this reduces to
\[
\distro{Triangle}(y,-1,1) = 1 - |y|.
\]
The file \url{src/models/basic_distributions/triangle.stan} contains
the following \Stan implementation of a sampler from 
$\distro{Triangle}(-1,1)$.
%
\begin{quote}
\begin{Verbatim}
parameters {
    real(-1,1) y;
}
model {
    lp__ <- lp__ + log1m(fabs(y));
}
\end{Verbatim}
\end{quote}
%
The single scalar parameter \code{y} is declared as lying in the
interval \code{(-1,1)}.  The log probability variable \code{lp\_\_} is
incremented with the joint log probabilty of all parameters, i.e.,
$\log \distro{Triangle}(y|-1,1)$.  This value is coded in \Stan as
\code{log1m(fabs(y))}.  The function \code{log1m} is is defined so
that \code{log1m(x)} has the same value as \code{log(1.0-x)}, but the
computation is faster, more accurate, and more stable.

The log probability variable \code{lp\_\_} is incremented in this
program rather than being set.  This is because the transform involved
for the bounded variable \code{y} of type \code{real(-1,1)} implicitly
adds a term to \code{lp\_\_} to adjust the log probability for the
transform (adding the log absolute derivative of the inverse
transform).

The constrained type \code{real(-1,1)} declared for \code{y} is
critical for correct sampling behavior.  If the constraint on \code{y}
is removed from the program, say by declaring \code{y} as having the
unconstrained scalar type \code{real}, the program would compile, but
it would produce arithmetic exceptions at run time when the sampler
explored values of \code{y} outside of $(-1,1)$.

Now suppose the log probability function were extended to all of
$\reals$ as follows by defining the probability to be \code{log(0.0)},
i.e., $-\infty$, for values outside of $(-1,1)$.
%
\begin{quote}
\begin{Verbatim}
    lp__ <- log(fmax(0.0,1 - fabs(y)));
\end{Verbatim}
\end{quote}
%
With the constraint on \code{y} in place, this is just a less
efficient, slower, and less arithmetically stable version of the
original program.  But if the constraint on \code{y} is removed, 
the model will compile and run without arithmetic errors, but will not
sample properly.%
%
\footnote{The problem is the (extremely!) light tails of the triangle
  distribution.  The standard \HMC and \NUTS samplers cannot get into the
  corners of the triangle properly.  Because the actual code declares
  \code{y} to be of type \code{real(-1,1)}, the inverse logit
  transform is applied to the unconstrained variable and its log
  absolute derivative added to the log probability.  The resulting
  distribution on the logit-transformed \code{y} is well behaved.  See
  \refchapter{variable-transforms} for more information on the actual
  transforms used.}


