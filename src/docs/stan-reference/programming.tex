





\part{Programming Techniques}\label{programming-techniques.part}



\chapter{Missing Data}

\section{Known and Unknown Quantities}

\BUGS and \R support mixed arrays of known and missing data.  In
\BUGS, known and unknown values may be mixed as long as every unknown
variable appears on the left-hand side of either an assignment or
sampling statement.  

\Stan treats variables declared in the \code{data} and
\code{transformed data} blocks as known and the variables in the
\code{parameters} block as unknown.

\subsection{Mixing Known and Unknown Quantities}

The \code{transformed parameters} block may be used to combine known
and unknown values.  The following program skeleton provides an example.
%
\begin{quote}
\begin{Verbatim}
data {
  int K;   
  int N_obs;               
  int N_miss;
  real y_obs[N_obs];
}
transformed data {
  int N;   
  N <- N_obs + N_miss;
}
parameters {
  real beta[K];
  real(0,) sigma;
  real y_miss[N_miss];
}
transformed parameters {
  real y[N];
  for (n in 1:N_obs)  
    y[n] <- y_obs[n];
  for (n in 1:N_miss) 
      y[n + N_obs] <- y_miss[n];
}
model {
  for (n in 1:N)
    y[n] ~ normal(mu,sigma);
}
\end{Verbatim}
\end{quote}
%
The transformed parameter array \code{y} is declared to be of size
\code{N}, the combined size of the observed and missing data arrays,
and then defined by copying the observed data array \code{y\_obs}
followed by the missing parameter array \code{y\_miss}.  In the model
block, the resulting mixed array \code{y} may be used like any other
variable.

Running the resulting program will sample values for the the missing
data \code{y\_miss} just like the other parameters \code{mu} and
\code{sigma}.

\subsection{Missing Data}

Although useful in some situations, creating a mixed transformed
parameter array is not the recommended way to program missing data
problems in \Stan.  The behavior is correct, but the computation is
wasteful.  Each parameter, be it declared in the \code{parameters} or
\code{transformed parameters} block, uses an algorithmic
differentiation variable which is more expensive in terms of memory
and gradient-calculation time than a simple data variable.
Furthermore, the copy takes up extra space, and the copy takes extra
time.

The recommended approach keeps the same \code{data} and
\code{parameters} blocks to define the observed and missing data.
%
\begin{quote}
\begin{Verbatim}
data {
  int N_obs;
  int N_miss;
  real y_obs[N_obs];
}
parameters {
  real mu;
  real(0,) sigma;
  real y_miss[N_miss];
}
model {
  for (n in 1:N_obs)
    y_obs[n] ~ normal(mu,sigma);
  for (n in 1:N_miss)
    y_miss[n] ~ normal(mu,sigma);
}
\end{Verbatim}
\end{quote}
%
There is no longer any transformed data.  Instead, the model contains
one loop over the observed data and one over the missing data.  This
model will more efficiently compute the same samples over the
distribution parameters and the missing data (it differs slightly in
not also providing the combined array \code{y} of transformed parameters).




\chapter{Truncated or Censored Data}



\chapter{Mixture Modeling}

Mixture models of an outcome assume that the outcome is drawn from one
of several distributions, the identity of which is controlled by a
categorical mixing distribution.  Mixture models typically have
multimodal densities with modes around the modes of the mixture
components.  

\section{Latent Discrete Parameterization}

One way to parameterize a mixture model is with a latent categorical
variable indicating which mixture component was responsible for the
outcome. For example, consider $K$ normal distributions with locations
$\mu_k \in \reals$ and scales $\sigma_k \in (0,\infty)$ mixed in the
proportions $\theta$ in the $K$-simplex.  For each outcome $y_n$ there
is a latent variable $z_n$ in $\setlist{1,\ldots,K}$,
%
\[
z_n \sim \distro{Categorical}(\theta).
\]
%
The variable $y_n$ is distributed according to the parameters
of the mixture component $z_n$, 
\[
y_n \sim \distro{Normal}(\mu_{z[n]},\sigma_{z[n]}).
\]
%
This model is not directly supported by \Stan because it involves a
discrete parameter $z$.  

\section{Summing out the Responsibility Parameter}

To implement the normal mixture model outlined in the previous
section in \Stan, the discrete parameters can be summed out of the
model. If $Y$ is a mixture of $K$ normal distributions with 
locations $\mu_k$ and scales $\sigma_k$ with mixing proportions
$\theta$ in the $K$-simplex, then 
\[
p_Y(y) = \sum_{k=1}^K \distro{Normal}(\mu_k,\sigma_k).
\]

For example, the mixture of $\code{Normal}(-1,2)$ and
$\code{Normal}(3,1)$ with mixing proportion $\theta =
(0.3,0.7)^{\top}$ can be implemented in \Stan as follows.
%
\begin{quote}
\begin{Verbatim}
parameters {
  real y;
}
model {
   lp__ <- log_sum_exp(log(0.3) + normal_log(y,-1,2),
                       log(0.7) + normal_log(y,3,1));
}
\end{Verbatim}
\end{quote}
%
The log probability term is derived by taking
\begin{eqnarray*}
\log p_Y(y) & = & \log \, \left( 0.3 \times \distro{Normal}(y|-1,2) \, + \,
  0.7 \times
  \distro{Normal}(y|3,1) \, \right)
\\[2pt]
& = & \log(\! \begin{array}[t]{l}
                 \exp(\log(0.3 \times \distro{Normal}(y|-1,2))) \\
                 + \exp(\log(0.7 \times \distro{Normal}(y|3,1))) \ )
              \end{array}
% \\[4pt]
% & = & \log( \! \begin{array}[t]{l}\exp(\log(0.3) + \log \distro{Normal}(y|-1,2))
%             \\
%            + \exp(\log(0.7) + \log \distro{Normal}(y|3,1)) \ )
%             \end{array}
\\[2pt]
& = & \mbox{logSumExp}(\! \begin{array}[t]{l}
                         \log(0.3) + \log \distro{Normal}(y|-1,2),
                         \\                  
                         \log(0.7) + \log \distro{Normal}(y|3,1) \ ).
                       \end{array}
\end{eqnarray*}

Given the scheme for representing mixtures, it may be moved to an
estimation setting, where the locations, scales, and mixture
components are unknown.  Further generalizing to an arbitrary number
of mixture components yields the following model.
%
\begin{quote}
\begin{Verbatim}
data {
  int(1,) K;           // number of mixture components
  int(1,) N;           // number of data points
  real y[N];           // observations
}
parameters {
  simplex(K) theta;    // mixing proportions
  real mu[K];          // locations of mixture components
  real(0,) sigma[K];   // scales of mixture components
}
model {
  real ps[K];          // temp for log component densities
  for (k in 1:K) {
    mu[k] ~ normal(0,10);
    sigma[k] ~ uniform(0,10);
  }
  for (n in 1:N) {
    for (k in 1:K) {
      ps[k] <- log(theta[k]) 
               + normal_log(y[n],mu[k],sigma[k]);
    }
    lp__ <- lp__ + log_sum_exp(ps);    
  }
}
\end{Verbatim}
\end{quote}
%
The model involves \code{K} mixture components and \code{N} data
points.  The mixing proportions are defined to be a unit $K$-simplex
using \code{simplex(K)}, the components distributions locations
\code{mu[k]} are unconstrained and their scales \code{sigma[k]}
constrained to be positive.  The model declares a local variable
\code{ps} of type \code{real[K]}, which is used to accumulate the
contributions by each mixture component.

The locations and scales are drawn from simple priors for the sake of
this example, but could be anything supported by \Stan in general,
including a hierarchical model.  

The main action is in the loop over data points \code{n}.  For each
such point, the log of $\theta_k \times
\distro{Normal}(y_n|\mu_k,\sigma_k)$ is calculated and added to the
array \code{ps}.  Then the log probability is incremented with the log
sum of exponentials of those values.





\chapter{Regression Models}

\Stan supports the full range of regression models from simple linear
regressions to multilevel generalized linear models.  Coding
regression models in \Stan is very much like coding them in \BUGS.

\section{Linear Regression}

The simplest linear regression model is the following, with a single
predictor and intercept term.

\begin{quote}
\begin{Verbatim}
data {
   int(0,) N;
   real x[N];
   real y[N];
}
parameters {
    real alpha;
    real beta;
    real(0,) epsilon;
}
model {
    for (n in 1:N)
        y[n] ~ normal(beta * x[n] + alpha, epsilon);
}
\end{Verbatim}
\end{quote}
%
This model has improper priors for the two regression coefficients
(intercept \code{alpha} and slope \code{beta}) and the noise term.

\section{Coefficient and Noise Priors}

There are several ways in which this model can be generalized.  
For example, weak priors can be assigned to the coefficients as follows.
%
\begin{quote}
\begin{Verbatim}
    alpha ~ normal(0,100);
    beta ~ normal(0,100);
    epsilon ~ uniform(0,100);
\end{Verbatim}
\end{quote}
%
More informative priors based the (half) Cauchy distribution are coded
as follows.
%
\begin{quote}
\begin{Verbatim}
    alpha ~ cauchy(0,2.5);
    beta ~ cauchy(0,2.5);
    epsilon ~ cauchy(0,2.5);
\end{Verbatim}
\end{quote}
%
The regression coefficients \code{alpha} and \code{beta} are
unconstrained, but \code{epsilon} must be positive and properly
requires the half-Cauchy distribution, which would be expressed in
\Stan using truncation as follows.
%
\begin{quote}
\begin{Verbatim}
    epsilon ~ cauchy(0,2.5) T(0,);
\end{Verbatim}
\end{quote}
%
Because the log probability function need not be normalized, the extra
factor of 2 is not needed in the model, so the simpler un-truncated
distribution suffices.

\section{Robust Noise Models}

The standard approach to linear regression is to model the noise
term $\epsilon$ as having a normal distribution.  From \Stan's
perspective, there is nothing special about normally distributed
noise.  For instance, robust regression can be accomodated by giving
the noise term a Student-$t$ distribution.  To code this in \Stan, the
sampling distribution is changed, as follows.
%
\begin{quote}
\begin{Verbatim}
data {
    ...
    real(0,) nu;
}
...
model {
    for (n in 1:N)
        y[n] ~ student_t(nu,0,sigma);
}
\end{Verbatim}
\end{quote}
%
The degrees of freedom constant \code{nu} is specified as data.
Alternatively, it could be hard coded into the model.

\section{Logistic and Probit Regression}

For binary outcomes, logistic or probit regression can be used.  A
logistic regression with one predictor and an intercept is coded as
follows. 
%
\begin{quote}
\begin{Verbatim}
data {
    int(0,) N;
    real x[N];
    int(0,1) y[N];
}
parameters {
    real alpha;
    real beta;
}
model {
    for (n in 1:N)
        y[n] ~ bernoulli(inv_logit(beta * x[n] + alpha));
} 
\end{Verbatim}
\end{quote}
%
The noise parameter is built into the Bernoulli here.  

Logistic regression is a kind of generalized linear model with binary
outcomes and the log odds (logit) link function.  The inverse of the
link function appears in the model.  

Other link functions may be used in the same way.  For example, probit
regression uses the cumulative normal distribution function, which is
typically written as 
\[
\Phi(x) = \int_{-\infty}^x \distro{Normal}(y|0,1) \, dy.
\]
%
The cumulative unit normal distribution function $\Phi$ is also known
as the error function and written as \code{erf(x)}.  The probit
regression model may be coded in \Stan as follows.
%
\begin{quote}
\begin{Verbatim}
        y[n] ~ bernoulli(erf(beta * x[n] + alpha));
\end{Verbatim}
\end{quote}

\section{Multi-Logit Regression}

Multiple outcome forms of logistic regression can be coded directly in
\Stan.  For instance, suppose there are $K$ possible outcomes for each
output variable $y_n$.  Also suppose that there is a $D$-dimensional
vector $x_n$ of predictors for $y_n$.  The multi-logit model with
improper flat priors on the coefficients is coded as follows.
%
\begin{quote}
\begin{Verbatim}
data {
   int(0,) N;
   int(1,K) y[N];
   row_vector(K) x[N];
}
parameters {
   matrix(N,K) beta;
}
model {
   for (n in 1:N)
       y[n] ~ categorical(softmax(x[n] * beta));
}
\end{Verbatim}
\end{quote}
%

\section{Ordered Logistic Regression}

Ordered logistic regression for an outcome $y_n \in
\setlist{1,\ldots,K}$ with predictors $x_n \in \reals^D$ is determined
by a single coefficient vector $\beta \in \reals^D$ along with a
sequence of cutpoints $c \in \reals^{D-1}$ sorted so that $c_d <
c_{d+1}$.  The discrete output is $k$ if the linear predictor $x_n
\beta$ falls between $c_{k-1}$ and $c_k$, assuming $c_0 = -\infty$ and
$c_K = \infty$.

The ordinal logistic model can be coded in \Stan using the
\code{ordered} data type for the cutpoints and the built-in
\code{ordered\_logistic} distribution.
%
\begin{quote}
\begin{Verbatim} 
data {
  int(1,) D;
  int(2,) K;
  int(0,) N;
  int(1,K) y[N];
  row_vector(D) x[N];
} 
parameters {
  matrix(D) beta;
  ordered(K-1) c;
} 
model {
  for (n in 1:N)
      y[n] ~ ordered_logistic(x[n] * beta, c);
}
\end{Verbatim}
\end{quote}
% 
The vector of cutpoints \code{c} is declared as \code{ordered(D-1)},
which guarantees that \code{c[k]} is less than \code{c[k+1]}.

An ordered probit model could be coded in a manner similar to the
\BUGS encoding of an ordered logistic model.
%
\begin{quote}
\begin{Verbatim}
data {
  int(1,) D;
  int(2,) K;
  int(0,) N;
  int(1,K) y[N];
  row_vector(D) x[N];
}
parameters {
  matrix(D) beta;
  ordered(K-1) c;
}
model {
  vector(K) theta;
  for (n in 1:N) {
      real eta;
      eta <- x[n] * beta;
      theta[1] = 1 - erf(eta - c(1));
      for (k in 2:(K-1))
          theta[k] = erf(eta - c(k-1)) - erf(eta - c(k));
      theta[K] = erf(eta - c(K-1));
      y[n] ~ categorical(theta);
  }
}
\end{Verbatim}
\end{quote}
%
The logistic model could be coded this way by replacing \code{erf}
with \code{inv\_logit}, though the built-in encoding is more efficient
and more numerically stable.  A small efficiency gain could be
achieved by computing the values \code{1 - erf(eta - c(d))} once and
storing them for re-use.

\section{Hierarchical Logistic Regression}

The simplest multilevel model is a hierarchical model in which the
data is grouped into $L$ levels.  An extreme approach would be to
completely pool all the data and estimate a common vector of
regression coefficients $\beta$.  At the other extreme, an approach
would no pooling assigns each level $l$ its own coefficient vector
$\beta_l$ that is estimated separately from the other levels.  A
hierarchical model is an intermediate solution where the degree of
pooling is determined by the data and a prior on the amount of
pooling.

Suppose each binary outcome $y_n \in \setlist{0,1}$ has an associated
level, $ll_n \in \setlist{1,\ldots,L}$.  Each outcome will also have
an associated predictor vector $x_n \in \reals^D$.  Each level $l$
gets its own coefficient vector $\beta_l \in \reals^D$.  The
hierarchical structure involves drawing the coefficients $\beta_{l,d}
\in \reals$ from a prior that is also estimated with the data.  This
hierarchically estimated prior determines the amount of pooling; if
the data in each level are very similar, the pooling will be strong,
but if it is very dissimilar, the pooling will be weak.

The following model encodes a hierarchical logistic regression model
with a hierarchical prior on the regression coefficients.
%
\begin{quote}
\begin{Verbatim}
data {
    int(1,) D;
    int(0,) N;
    int(1,) L;
    int(0,1) y[N];
    int(1,L) ll[N];
}
parameters {
    real mu[D];
    real(0,) sigma[D];
    vector(D) beta[L];
}
model {
    for (d in 1:D) {
        mu[d] ~ normal(0,100);
        sigma[d] ~ uniform(0,1000);
        for (l in 1:L)
            beta[l,d] ~ normal(mu[d],sigma[d]);
    }
    for (n in 1:N)
        y[n] ~ bernoulli(inv_logit(x[n] * beta[ll[n]]));
}
\end{Verbatim}
\end{quote}   






\chapter{Order Constraints}



\chapter{Custom Probability  Functions}%
\label{custom-probability-functions.chapter}


Custom distributions may also be implemented directly within \Stan's
programming language.  A simple example is the triangle distribution.
If $\alpha \in \reals$ and $\beta \in (\alpha,\infty)$, then
$y \in (\alpha,\beta)$ has a density defined as follows.
\[
\distro{Triangle}(y | \alpha,\beta)
= 
\frac{2}{\beta - \alpha}
\
\left(
1 - 
\left|
y - \frac{\alpha + \beta}{\beta - \alpha}
\right|
\right)
\]
%
If $\alpha = -1$, $\beta = 1$, and $y \in (-1,1)$, this reduces to
\[
\distro{Triangle}(y,-1,1) = 1 - |y|.
\]
The file \url{src/models/basic_distributions/triangle.stan} contains
the following \Stan implementation of a sampler from 
$\distro{Triangle}(-1,1)$.
%
\begin{quote}
\begin{Verbatim}
parameters {
    real(-1,1) y;
}
model {
    lp__ <- lp__ + log1m(fabs(y));
}
\end{Verbatim}
\end{quote}
%
The single scalar parameter \code{y} is declared as lying in the
interval \code{(-1,1)}.  The log probability variable \code{lp\_\_} is
incremented with the joint log probabilty of all parameters, i.e.,
$\log \distro{Triangle}(y|-1,1)$.  This value is coded in \Stan as
\code{log1m(fabs(y))}.  The function \code{log1m} is is defined so
that \code{log1m(x)} has the same value as \code{log(1.0-x)}, but the
computation is faster, more accurate, and more stable.

The log probability variable \code{lp\_\_} is incremented in this
program rather than being set.  This is because the transform involved
for the bounded variable \code{y} of type \code{real(-1,1)} implicitly
adds a term to \code{lp\_\_} to adjust the log probability for the
transform (adding the log absolute derivative of the inverse
transform).

The constrained type \code{real(-1,1)} declared for \code{y} is
critical for correct sampling behavior.  If the constraint on \code{y}
is removed from the program, say by declaring \code{y} as having the
unconstrained scalar type \code{real}, the program would compile, but
it would produce arithmetic exceptions at run time when the sampler
explored values of \code{y} outside of $(-1,1)$.

Now suppose the log probability function were extended to all of
$\reals$ as follows by defining the probability to be \code{log(0.0)},
i.e., $-\infty$, for values outside of $(-1,1)$.
%
\begin{quote}
\begin{Verbatim}
    lp__ <- log(fmax(0.0,1 - fabs(y)));
\end{Verbatim}
\end{quote}
%
With the constraint on \code{y} in place, this is just a less
efficient, slower, and less arithmetically stable version of the
original program.  But if the constraint on \code{y} is removed, 
the model will compile and run without arithmetic errors, but will not
sample properly.%
%
\footnote{The problem is the (extremely!) light tails of the triangle
  distribution.  The standard \HMC and \NUTS samplers cannot get into the
  corners of the triangle properly.  Because the actual code declares
  \code{y} to be of type \code{real(-1,1)}, the inverse logit
  transform is applied to the unconstrained variable and its log
  absolute derivative added to the log probability.  The resulting
  distribution on the logit-transformed \code{y} is well behaved.  See
  \refchapter{variable-transforms} for more information on the actual
  transforms used.}


